{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d19912c9-5889-45bf-83a4-67c794e5a953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement json (from versions: none)\n",
      "ERROR: No matching distribution found for json\n"
     ]
    }
   ],
   "source": [
    "!pip install json -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830973db-d231-4a9d-80b9-7c4304ed30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "data = nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce326ad-266c-4571-b917-3544194f9f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kebab\\AppData\\Roaming\\nltk_data\\corpora\\twitter_samples\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\Kebab\\AppData\\Roaming\\nltk_data\\corpora\\twitter_samples'\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6727a106-c313-45e5-9e8a-3a1a32ff136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "import numpy as np\n",
    "\n",
    "def load_tweets(path):    \n",
    "    '''\n",
    "    This function reads the email from directory\n",
    "    Args:\n",
    "        path: name of the json file\n",
    "    Returns:\n",
    "        data: list of tweets in the file\n",
    "    '''\n",
    "    direct = r'C:\\Users\\Kebab\\AppData\\Roaming\\nltk_data\\corpora\\twitter_samples'\n",
    "    data = []\n",
    "    with open(direct+'\\\\' + path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line)['text'])\n",
    "            \n",
    "    return data\n",
    "def parse_tweet(tweets):\n",
    "    '''\n",
    "    This function parse the tweet to split into words\n",
    "    Args:\n",
    "        tweets: list of strings where each string contains a single tweet\n",
    "    Returns:\n",
    "        tweet_word: list of lists where each list contains word in single tweet\n",
    "        \n",
    "    '''\n",
    "    tweet_word = []\n",
    "    for tweet in tweets:\n",
    "        tweet_word.append({word for word in tweet.lower().split(' ') if not word.startswith('@')})\n",
    "    return tweet_word\n",
    "    \n",
    "def create_dict(data):\n",
    "    '''\n",
    "    This function will count the number of times each word occurs in different tweets\n",
    "    Args:\n",
    "        data: list of tweet words\n",
    "    Returns:\n",
    "        d: dictionary of word, count pairs where count is how tweets contains the word\n",
    "    '''\n",
    "    d = {}\n",
    "    for tweet in data:\n",
    "        for word in tweet:\n",
    "            wrd = d.get(word, None)\n",
    "            if wrd is not None:\n",
    "                d[word] += 1\n",
    "            else:\n",
    "                d[word] = 1\n",
    "    return d\n",
    "\n",
    "def predict(tweets):\n",
    "    '''\n",
    "    This function performs the naive Bayes calculation\n",
    "    Args:\n",
    "        tweets: list of tweets for making prediction\n",
    "    Returns:\n",
    "        ans: list of values(0, 1) wether the tweet is + or -.\n",
    "    '''\n",
    "    ans = []\n",
    "    logit_p, logit_n = np.log(0.5), np.log(0.5) \n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            p_cnt, n_cnt = pos_dic.get(word, 0), neg_dic.get(word, 0)\n",
    "            if p_cnt == 0 and n_cnt == 0:\n",
    "                continue\n",
    "            else:\n",
    "                logit_p += np.log((p_cnt + 1)/(v + len(pos_dic)))\n",
    "                logit_n += np.log((n_cnt + 1)/(v + len(neg_dic)))          \n",
    "        ans.append(1 if logit_p > logit_n else 0)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def metrics(neg, pos):\n",
    "    '''\n",
    "    This function calculates the scores of the metrics\n",
    "    Args:\n",
    "        neg: Prediction of the model for the negative test tweets\n",
    "        pos: Prediction of the model for the positive test tweets\n",
    "    Returns:\n",
    "        dict: precision, recall, f1 scores\n",
    "    '''\n",
    "    tp = sum(pos)\n",
    "    fn = len(pos) - sum(pos)\n",
    "    fp = sum(neg)\n",
    "    tn = len(neg) - sum(neg)\n",
    "    accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "    recall = tp/(tp + fn)\n",
    "    precision = tp/(tp + fp)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return {\"precision\":precision, 'recall':recall, 'f1':f1}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1430c0a6-53c1-47bb-8611-8ee40bcd822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the tweets from the directory\n",
    "neg_path = 'negative_tweets.json'\n",
    "pos_path = 'positive_tweets.json'\n",
    "neg_tweet = random.sample(load_tweets(neg_path), 500)\n",
    "pos_tweet = random.sample(load_tweets(pos_path), 500)\n",
    "\n",
    "#parse each tweet into a list of words\n",
    "neg_count, pos_count = parse_tweet(neg_tweet), parse_tweet(pos_tweet)\n",
    "#split the training and testing tweet\n",
    "neg_train = neg_count[:400]\n",
    "pos_train = pos_count[:400]\n",
    "neg_test = neg_count[400:]\n",
    "pos_test = pos_count[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61e7d928-eb67-46e5-ae04-0a68ae5e02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the counting of the words for positive and negative class\n",
    "pos_dic, neg_dic = create_dict(pos_train,), create_dict(neg_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08663bd0-b5cb-4beb-afb5-baf9d679c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the total unique words for laplace smoothing\n",
    "v = len(set(pos_dic.keys()) |  set(neg_dic.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d00fe715-1eee-4ff0-b887-33aa1e66f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a prediction on the positive and negative test tweets\n",
    "neg_pred, pos_pred = predict(neg_test), predict(pos_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ce4b10a-d904-4b51-bebe-cd8ee4e4553b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 1.0, 'recall': 1.0, 'f1': 1.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the metrics\n",
    "metrics(neg_pred, pos_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91deb2-dd3b-4e78-8db0-a91655c62030",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b3386-688c-45d7-92ef-f993c2e6afd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
